{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"print('Ready for inference')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Since Internet is disabled for inference, I have cloned **ObjectDetectionfastai** library and saved in kaggle datasets.\n* Install just from there\n* Since we are just making predictions we are good to go without a **GPU**.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install ../input/objectdetectionfastai/ObjectDetection-master/ObjectDetection-master","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom collections import defaultdict\nimport os\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.callbacks import *\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport matplotlib.image as immg\nfrom sklearn.model_selection import StratifiedKFold,KFold\n\nfrom object_detection_fastai.helper.object_detection_helper import *\nfrom object_detection_fastai.loss.RetinaNetFocalLoss import RetinaNetFocalLoss\nfrom object_detection_fastai.models.RetinaNet import RetinaNet\nfrom object_detection_fastai.callbacks.callbacks import BBLossMetrics, BBMetrics, PascalVOCMetric","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\",)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style('darkgrid')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = Path('/kaggle/input/global-wheat-detection');path.ls()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"[PosixPath('/kaggle/input/global-wheat-detection/train.csv'),\n PosixPath('/kaggle/input/global-wheat-detection/train'),\n PosixPath('/kaggle/input/global-wheat-detection/test'),\n PosixPath('/kaggle/input/global-wheat-detection/sample_submission.csv')]"},"metadata":{}}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv(path/'train.csv')","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tr = train.image_id.value_counts()\ntr = pd.DataFrame({'image_id':tr.index,'wheat_count':tr.values})\ntr = tr.sample(frac=1.,random_state=2020).reset_index(drop=True)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample = pd.read_csv('../input/global-wheat-detection/sample_submission.csv')","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lbl_img(train):\n    wheat2bbox = {}\n    train['label'] = 'wheat'\n    grp = train.image_id.unique()\n    tr_gr = train.groupby(['image_id'])\n    from tqdm.notebook import tqdm\n    for i in tqdm(range(len(grp))):\n        name = str(grp[i]) + '.jpg'\n        bbox = []\n        temp_b = []\n        temp = tr_gr.get_group(grp[i])\n        tt = temp.loc[:,'bbox'].values\n        for j in range(len(temp)):\n            t = tt[j][1:-1].split(',')\n            t = [float(x) for x in t]  # x,y, width, height\n            # Currently our coordinates are x,y,w,h and we want x1,y1,x2,y2\n            # To convert it, we need to add our width and height to the respective x and y.\n            t[2],t[3] = t[0]+t[2],t[1]+t[3]  \n            # To achieve x2,y2 those we simply add width to x and height to y :\n            # x2 = x + w and y2 = y + h\n            t1 = [t[1],t[0],t[3],t[2]]   # inverse in fromat w,h for fastai\n            temp_b.append(t1)\n        bbox.append(temp_b)\n        bbox.append(['wheat']*len(temp))\n        wheat2bbox[name] = bbox\n    return wheat2bbox","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wheat2bbox = get_lbl_img(train)","execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=3373.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9a35a5958514d95a50957d5937b39c0"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Test DataBunch"},{"metadata":{"trusted":true},"cell_type":"code","source":"get_y_func = lambda o: wheat2bbox[Path(o).name] ","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ts = (ObjectItemList.from_df(sample,path, folder = 'test' , suffix = '.jpg',cols='image_id'))","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = (ObjectItemList.from_df(tr,path, folder = 'train' , suffix = '.jpg',cols='image_id')\n        #Where are the images? ->\n        .split_by_rand_pct(0.2)                          \n        #How to split in train/valid? -> randomly with the default 20% in valid\n        .label_from_func(get_y_func)\n        #How to find the labels? -> use get_y_func on the file name of the data\n        .transform(size=512)\n        .add_test(ts)\n        #Data augmentation? -> Standard transforms; also transform the label images\n        .databunch(bs=2, collate_fn=bb_pad_collate))   ","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"size = 512","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"anchors = create_anchors(sizes=[(32,32),(16,16),(8,8),(4,4)], ratios=[0.5, 1, 2], scales=[0.35, 0.55, 0.75, 1, 1.25, 1.45])","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_dir = Path('../input/fastai-v1-global-wheat-detection-tutorial')","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = load_learner(model_dir,'gwheat.pkl',test=ts)","execution_count":17,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'object_detection_fastai'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-fb0e2822786b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'gwheat.pkl'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mload_learner\u001b[0;34m(path, file, test, tfm_y, **db_kwargs)\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;34m\"Load a `Learner` object saved with `export_state` in `path/file` with empty data, optionally add `test` and load on `cpu`. `file` can be file-like (file or buffer)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m     \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_pathlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelLists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'object_detection_fastai'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_id = sample.image_id.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_output(item,bboxs_tot,scores_tot):\n    fig,ax = plt.subplots(figsize=(10,10))\n    ax.imshow(image2np(item.data))\n    plt.axis('off')\n    area_max = 512**2/5 \n    for bbox, c in zip(bboxs_tot[0], scores_tot[0].numpy()):\n        txt = 'wheat, {0:.4f}'.format(c)\n        if bbox[2]*bbox[3] <= area_max:\n            draw_rect(ax, [bbox[1],bbox[0],bbox[3],bbox[2]], text=txt,text_size=12,color='red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def process_preds_show(item,clas,bboxs,show_img,cnt,i):\n    detect_thresh=0.4   # set your own detection threshold\n    nms_thresh=0.1\n    pred_string = []\n    scores_tot = []\n    bboxs_tot = []\n    show_img = True if i<cnt else False\n    for clas_pred, bbox_pred in list(zip(clas, bboxs)):\n        bbox_pred, scores, preds = process_output(clas_pred, bbox_pred, anchors, detect_thresh)\n        if bbox_pred is not None:\n            to_keep = nms(bbox_pred, scores, nms_thresh)\n            bbox_pred, preds, scores = bbox_pred[to_keep].cpu(), preds[to_keep].cpu(), scores[to_keep].cpu()\n        t_sz = torch.Tensor([size])[None].cpu()\n        if bbox_pred is not None:\n            bbox_pred = to_np(rescale_boxes(bbox_pred, t_sz))\n                # change from center to top left\n            bbox_pred[:, :2] = bbox_pred[:, :2] - bbox_pred[:, 2:] / 2\n            bboxs_tot.append(bbox_pred)\n            scores_tot.append(scores)\n    if show_img:\n        show_output(item,bboxs_tot,scores_tot)\n    area_max = (1024**2)/5\n    for s,bbx in zip(scores_tot[0].numpy(),bboxs_tot[0]):\n        bbx = [int(round(x)) for x in bbx*2]\n        if bbx[2]*bbx[3] <= area_max :\n            res = \"{0:.4f} {1} {2} {3} {4}\".format(s,bbx[1],bbx[0],bbx[3],bbx[2])\n            pred_string.append(res)\n    return pred_string","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_prediction(show_img=True,cnt=10): \n    # Set show img True to see img or else false for bboxs only, cnt for number of images to show\n    preds_str = {}\n    for i in range(len(data.test_ds)):\n        item = learn.data.test_ds[i][0]  #Pick one image\n        batch = learn.data.one_item(item)\n        clas,bboxs,xtr = learn.pred_batch(batch=batch)\n        prd = process_preds_show(item,clas,bboxs,show_img,cnt,i) \n        preds_str[image_id[i]] = \" \".join(prd)\n    return preds_str","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* A look at first test images"},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = get_prediction(cnt=0)\n# Set False to not show images\n# Regardless of that it will give prediction string ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit = pd.DataFrame.from_dict(prediction,orient='index').reset_index()\nsubmit.columns = ['image_id','PredictionString']\nsubmit.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submit.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}